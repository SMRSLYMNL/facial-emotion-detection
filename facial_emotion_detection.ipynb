{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we add imports that we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from keras.api.models import Sequential \n",
    "from keras.api.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten \n",
    "from keras.api.optimizers import Adam \n",
    "from keras.api.optimizers.schedules import ExponentialDecay\n",
    "from keras._tf_keras.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.api.models import model_from_json \n",
    "import cv2\n",
    "import cvlib as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will define Enums and methods that we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Enum):\n",
    "    CKPlus = 'ck+'\n",
    "    FER2013 = 'fer_2013'\n",
    "    MMA = 'mma'\n",
    "\n",
    "\n",
    "class DataType(Enum):\n",
    "    TRAINING = 'train'\n",
    "    TESTING = 'test'\n",
    "\n",
    "\n",
    "def get_emotion_dict(dataset=Dataset.FER2013):\n",
    "    if(dataset == Dataset.CKPlus):\n",
    "        return {\n",
    "            0: 'Angry',\n",
    "            1: 'Disgusted',\n",
    "            2: 'Fearful',\n",
    "            3: 'Happy',\n",
    "            4: 'Sad',\n",
    "            5: 'Surprised'\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        0: 'Angry',\n",
    "        1: 'Disgusted',\n",
    "        2: 'Fearful',\n",
    "        3: 'Happy',\n",
    "        4: 'Neutral',\n",
    "        5: 'Sad',\n",
    "        6: 'Surprised'\n",
    "    }\n",
    "\n",
    "\n",
    "def get_data_path(dataset=Dataset.FER2013, data_type=DataType.TRAINING):\n",
    "    return f'datasets/{dataset.value}/{data_type.value}/'\n",
    "\n",
    "\n",
    "def get_model_json_path(dataset):\n",
    "    folderPath = Path(f'models/{dataset.value}')\n",
    "    folderPath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    path = Path(f'{folderPath}/emotion_detection_model.json')\n",
    "\n",
    "    return path\n",
    "\n",
    "\n",
    "def get_model_h5_path(dataset):\n",
    "    folderPath = Path(f'models/{dataset.value}')\n",
    "    folderPath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    path = Path(f'{folderPath}/emotion_detection_model.weights.h5')\n",
    "    return path\n",
    "\n",
    "\n",
    "def get_emotions(emotion_prediction):\n",
    "    list_of_weights = emotion_prediction[0]\n",
    "\n",
    "    print(\"list_of_weights\", list_of_weights)\n",
    "\n",
    "    list_of_percentages = [weight * 100 for weight in list_of_weights]\n",
    "    \n",
    "    emotion_dict = get_emotion_dict()\n",
    "\n",
    "    emotions = {\n",
    "        emotion_dict[index]: percentage for index, percentage in enumerate(list_of_percentages)\n",
    "    }\n",
    "\n",
    "    print(\"emotions\", emotions)\n",
    "\n",
    "    dominant_emotion = max(emotions, key=emotions.get)\n",
    "\n",
    "    print(\"dominant_emotion\", dominant_emotion)\n",
    "\n",
    "    return {'dominant_emotion': dominant_emotion, 'emotions': emotions}\n",
    "\n",
    "\n",
    "dataset = Dataset.FER2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we define the CNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model = Sequential() \n",
    "\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu',  \n",
    "                        input_shape=(48, 48, 1))) \n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu')) \n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "emotion_model.add(Dropout(0.25)) \n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu')) \n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu')) \n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2))) \n",
    "emotion_model.add(Dropout(0.25)) \n",
    "\n",
    "emotion_model.add(Flatten()) \n",
    "emotion_model.add(Dense(1024, activation='relu')) \n",
    "emotion_model.add(Dropout(0.5)) \n",
    "emotion_model.add(Dense(7, activation='softmax')) \n",
    "\n",
    "emotion_model.summary() \n",
    "\n",
    "cv2.ocl.setUseOpenCL(False) \n",
    "\n",
    "initial_learning_rate = 0.0001\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps=100000,  \n",
    "                            decay_rate=0.96) \n",
    "\n",
    "optimizer = Adam(learning_rate=lr_schedule) \n",
    "\n",
    "emotion_model.compile(loss='categorical_crossentropy', optimizer=optimizer,  \n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use ImageDataGenerator to generate data for both train and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = ImageDataGenerator(rescale=1./255) \n",
    "validation_data_gen = ImageDataGenerator(rescale=1./255) \n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_generator = train_data_gen.flow_from_directory( \n",
    "    get_data_path(dataset=dataset), \n",
    "    target_size=(48, 48), \n",
    "    batch_size=batch_size, \n",
    "    color_mode=\"grayscale\", \n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = validation_data_gen.flow_from_directory( \n",
    "    get_data_path(dataset=dataset, data_type=DataType.TESTING), \n",
    "    target_size=(48, 48), \n",
    "    batch_size=batch_size, \n",
    "    color_mode=\"grayscale\", \n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train the CNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_count = train_generator.samples\n",
    "test_sample_count = validation_generator.samples\n",
    "\n",
    "emotion_model_info = emotion_model.fit(\n",
    "    train_generator,\n",
    "    batch_size=batch_size,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy and loss evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.evaluate(validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing accuracy and loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the accuracy and loss values from model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = emotion_model_info.history['accuracy'] \n",
    "val_accuracy = emotion_model_info.history['val_accuracy'] \n",
    "loss = emotion_model_info.history['loss'] \n",
    "val_loss = emotion_model_info.history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we plot the accuracy and loss graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy graph \n",
    "plt.subplot(1, 2, 1) \n",
    "plt.plot(accuracy, label='accuracy') \n",
    "plt.plot(val_accuracy, label='val accuracy') \n",
    "plt.title('Accuracy Graph') \n",
    "plt.xlabel('Epochs') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.legend() \n",
    "  \n",
    "# loss graph \n",
    "plt.subplot(1, 2, 2) \n",
    "plt.plot(loss, label='loss') \n",
    "plt.plot(val_loss, label='val loss') \n",
    "plt.title('Loss Graph') \n",
    "plt.xlabel('Epochs') \n",
    "plt.ylabel('Loss') \n",
    "plt.legend() \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving model in JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = emotion_model.to_json() \n",
    "with open(get_model_json_path(dataset=dataset), \"w+\") as json_file: \n",
    "    json_file.write(model_json) \n",
    "  \n",
    "# save trained model weight in .h5 file \n",
    "emotion_model.save_weights(get_model_h5_path(dataset=dataset)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take model as JSON and create model using it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open(get_model_json_path(dataset=dataset), 'r') \n",
    "loaded_model_json = json_file.read() \n",
    "json_file.close() \n",
    "emotion_model = model_from_json(loaded_model_json) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model with the live data (using camera recording):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start capturing video from the default camera (usually the built-in webcam)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    try:\n",
    "        # Detect faces in the frame\n",
    "        faces, confidences = cv.detect_face(frame)\n",
    "    except Exception as e:\n",
    "        print('Exception on face detection:', e)\n",
    "\n",
    "    # Loop through detected faces\n",
    "    for face, confidence in zip(faces, confidences):\n",
    "        try:\n",
    "            (start_x, start_y, end_x, end_y) = face\n",
    "\n",
    "            # Crop the face from the frame\n",
    "            face_crop = frame[start_y:end_y, start_x:end_x]\n",
    "\n",
    "            # Resize the face for prediction (adjust the size as needed)\n",
    "            face_resize = cv2.resize(face_crop, (48, 48))\n",
    "\n",
    "            # Grayscale the image to fit the training data\n",
    "            face_grayscale = cv2.cvtColor(face_resize, cv2.COLOR_BGR2GRAY) \n",
    "\n",
    "            # Convert the image to NDArray\n",
    "            face_data = np.expand_dims(np.expand_dims(face_grayscale, -1), 0)\n",
    "\n",
    "            # Perform emotion prediction\n",
    "            result = emotion_model.predict(face_data)\n",
    "\n",
    "            # Make result readable\n",
    "            prediction = get_emotions(result)\n",
    "\n",
    "            # Get the dominant emotion\n",
    "            dominant_emotion = prediction['dominant_emotion']\n",
    "            emotions = prediction['emotions']\n",
    "\n",
    "            # Label to show the dominant emotion\n",
    "            dominant_emotion_label = \"Emotion: {}\".format(dominant_emotion)\n",
    "\n",
    "            # Face outline\n",
    "            cv2.rectangle(frame, (start_x, start_y), (end_x, end_y), (0, 150, 0), 2)\n",
    "\n",
    "            # Analysis Details\n",
    "            overlay = frame.copy()\n",
    "\n",
    "            # Transparent rectangle\n",
    "            cv2.rectangle(overlay, (end_x, start_y), (end_x + 230, start_y + 200), (192, 192, 192), -1)\n",
    "            alpha = 0.5\n",
    "\n",
    "            # Overlay transparent rectangle over the image\n",
    "            frame = cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0)\n",
    "\n",
    "            # Put top label\n",
    "            cv2.putText(frame, dominant_emotion_label, (start_x, start_y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "            # Put emotion list as text\n",
    "            y_offset = 25\n",
    "            for key, val in emotions.items():\n",
    "                cv2.putText(frame, \"{}: {:.2f}\".format(key, val), (end_x + 10, start_y + y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "                y_offset += 25\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Exception on face analysis:', e)\n",
    "\n",
    "    try:\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Emotion Detection', frame)\n",
    "\n",
    "        # Break the loop if 'q' key is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print('Exception on displaying results:', e)\n",
    "\n",
    "# Release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
